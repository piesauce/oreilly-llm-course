{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piesauce/oreilly-llm-course/blob/main/context_engineering_oreilly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22026ee8",
      "metadata": {
        "id": "22026ee8"
      },
      "source": [
        "# Context Engineering for Long Conversations (Dynamic Context Swapping)\n",
        "\n",
        "This Colab notebook demonstrates a practical **context engineering** loop:\n",
        "\n",
        "- Keep a long-running conversation with an OpenAI model\n",
        "- Maintain multiple context layers (**instructions**, **running summary**, **memory**, **retrieved docs**, **recent turns**)\n",
        "- Automatically **swap data in/out of the model context** so the chat can continue for a long time without dumping the entire transcript each turn\n",
        "- Inspect **the exact context payload** being sent to the model at any time (and even diff it turn-to-turn)\n",
        "\n",
        "> Safety note: donâ€™t paste secrets or sensitive personal data into prompts unless youâ€™re comfortable sending them to the API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9990626c",
      "metadata": {
        "id": "9990626c"
      },
      "source": [
        "## 1) Install & set up\n",
        "\n",
        "This notebook uses the OpenAI **Responses API** via the official Python SDK.\n",
        "\n",
        "- You'll set `OPENAI_API_KEY` (you can paste it, or use Colab Secrets / environment variables).\n",
        "- Then we'll build a `ContextEngine` that:\n",
        "  - stores conversation turns\n",
        "  - summarizes older turns into a running summary\n",
        "  - stores durable \"memory\" facts\n",
        "  - ingests large documents and retrieves only relevant snippets\n",
        "  - assembles an *explicit* context packet each turn and shows it to you\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2d265d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2d265d2",
        "outputId": "96901bbc-23cd-41ba-f506-225577bff314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m463.6/463.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.50.0 requires pydantic<=2.12.3,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Colab: install the SDK + a few tiny helpers\n",
        "!pip -q install --upgrade openai pydantic scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8eba06",
      "metadata": {
        "id": "3d8eba06"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# Option A: already set as an env var\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    # Option B: paste it here (input is hidden)\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OPENAI_API_KEY: \")\n",
        "\n",
        "client = OpenAI()\n",
        "print(\"OpenAI client ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48624759",
      "metadata": {
        "id": "48624759"
      },
      "source": [
        "## 2) Configuration\n",
        "\n",
        "Choose model IDs you have access to. The defaults below use a recent general-purpose model.\n",
        "\n",
        "We also set a **max input token budget** for what we send each turn (instructions + context packet + recent messages).\n",
        "The engine will compact/summarize and/or reduce retrieved snippets to stay under the budget.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbbcb1eb",
      "metadata": {
        "id": "bbbcb1eb"
      },
      "outputs": [],
      "source": [
        "# --- Models ---\n",
        "MODEL = \"gpt-5.2\"           # main chat model\n",
        "SUMMARIZER_MODEL = MODEL    # used to summarize older chat history\n",
        "MEMORY_MODEL = MODEL        # used to extract durable memory facts\n",
        "\n",
        "# --- Context policy knobs ---\n",
        "MAX_INPUT_TOKENS = 6000         # how much context we *send* each turn (not the model's full context window)\n",
        "KEEP_LAST_MESSAGES = 10         # keep this many of the most recent messages verbatim (user+assistant count)\n",
        "RETRIEVED_SNIPPETS = 4          # how many doc/memory snippets to retrieve and swap into context each turn\n",
        "PINNED_MEMORY_LIMIT = 20        # cap pinned memories (oldest/least important are dropped)\n",
        "AUTO_MEMORY = True              # let the model extract durable \"memory\" facts from turns\n",
        "TOKEN_COUNTING = True           # uses the token counting endpoint (extra API call) for exact input sizes\n",
        "SHOW_CONTEXT_EACH_TURN = False  # if True, prints context after every user turn\n",
        "\n",
        "print(\"Config loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ffdac48",
      "metadata": {
        "id": "9ffdac48"
      },
      "source": [
        "## 3) The ContextEngine\n",
        "\n",
        "This is the heart of the demo.\n",
        "\n",
        "### Layers we manage\n",
        "\n",
        "1. **Base instructions** (stable)\n",
        "2. **Running summary** of older conversation turns (compressed)\n",
        "3. **Durable memory** (facts/preferences/goals)\n",
        "4. **Large external docs** (ingested once, then retrieved by relevance)\n",
        "5. **Recent chat turns** (kept verbatim)\n",
        "\n",
        "Each turn we assemble an explicit *context packet* and pass it via `instructions=...` along with the recent messages via `input=[...]`.\n",
        "We also store the last payload so you can call `engine.show_context()` any time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "487910a3",
      "metadata": {
        "id": "487910a3"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "import difflib\n",
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Small utilities\n",
        "# -----------------------------\n",
        "\n",
        "def _now_iso() -> str:\n",
        "    return time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "\n",
        "def chunk_text(text: str, max_chars: int = 1200, overlap: int = 150) -> List[str]:\n",
        "    \"\"\"Chunk large text into overlapping slices (simple + robust for demos).\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    if len(text) <= max_chars:\n",
        "        return [text]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(len(text), start + max_chars)\n",
        "        chunk = text[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "        if end >= len(text):\n",
        "            break\n",
        "        start = max(0, end - overlap)\n",
        "    return chunks\n",
        "\n",
        "def format_messages(messages: List[Dict[str, str]]) -> str:\n",
        "    lines = []\n",
        "    for m in messages:\n",
        "        role = m.get(\"role\", \"?\")\n",
        "        content = m.get(\"content\", \"\")\n",
        "        lines.append(f\"{role.upper()}: {content}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def safe_truncate(s: str, max_chars: int = 2500) -> str:\n",
        "    s = s or \"\"\n",
        "    if len(s) <= max_chars:\n",
        "        return s\n",
        "    head = s[: max_chars // 2]\n",
        "    tail = s[-max_chars // 2 :]\n",
        "    return head + \"\\n... (truncated) ...\\n\" + tail\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Structured memory extraction schema\n",
        "# -----------------------------\n",
        "\n",
        "class MemoryFact(BaseModel):\n",
        "    \"\"\"A single durable fact worth remembering.\"\"\"\n",
        "    category: str = Field(\n",
        "        description=\"One of: identity, preference, project, constraint, other\"\n",
        "    )\n",
        "    text: str = Field(description=\"The memory as a short, standalone sentence.\")\n",
        "    importance: int = Field(ge=1, le=5, description=\"1=low, 5=high\")\n",
        "\n",
        "class MemoryDelta(BaseModel):\n",
        "    \"\"\"What to add to memory after a turn.\"\"\"\n",
        "    add: List[MemoryFact] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Core engine\n",
        "# -----------------------------\n",
        "\n",
        "@dataclass\n",
        "class KnowledgeChunk:\n",
        "    id: str\n",
        "    kind: str   # \"doc\" or \"memory\"\n",
        "    text: str\n",
        "    source: str\n",
        "    created_at: str = field(default_factory=_now_iso)\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "class ContextEngine:\n",
        "    \"\"\"A tiny, inspectable context-management layer for long conversations.\n",
        "\n",
        "    Key idea: keep the conversation going by *swapping* context in/out:\n",
        "      - Summarize older turns into a running summary (swap out transcript -> swap in summary)\n",
        "      - Store durable memory facts (swap in small facts instead of repeating)\n",
        "      - Ingest large docs and retrieve only the top-K relevant snippets per turn\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        client,\n",
        "        model: str,\n",
        "        summarizer_model: Optional[str] = None,\n",
        "        memory_model: Optional[str] = None,\n",
        "        max_input_tokens: int = 6000,\n",
        "        keep_last_messages: int = 10,\n",
        "        retrieved_snippets: int = 4,\n",
        "        pinned_memory_limit: int = 20,\n",
        "        auto_memory: bool = True,\n",
        "        token_counting: bool = True,\n",
        "        show_context_each_turn: bool = False,\n",
        "    ):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.summarizer_model = summarizer_model or model\n",
        "        self.memory_model = memory_model or model\n",
        "\n",
        "        # policy knobs\n",
        "        self.max_input_tokens = max_input_tokens\n",
        "        self.keep_last_messages = keep_last_messages\n",
        "        self.retrieved_snippets = retrieved_snippets\n",
        "        self.pinned_memory_limit = pinned_memory_limit\n",
        "        self.auto_memory = auto_memory\n",
        "        self.token_counting = token_counting\n",
        "        self.show_context_each_turn = show_context_each_turn\n",
        "\n",
        "        # stable instruction (you can edit this!)\n",
        "        self.base_instructions = (\n",
        "            \"You are a helpful assistant.\\n\"\n",
        "            \"Be accurate. Ask clarifying questions when needed.\\n\"\n",
        "            \"When given a Context Packet, use it as your source of truth for user preferences and prior decisions.\"\n",
        "        )\n",
        "\n",
        "        # mutable state\n",
        "        self.summary: str = \"\"\n",
        "        self.recent_messages: List[Dict[str, str]] = []   # [{role, content}]\n",
        "        self.memories: List[Dict[str, Any]] = []          # durable facts (some pinned)\n",
        "        self.docs: List[KnowledgeChunk] = []              # large doc chunks\n",
        "        self._vectorizer: Optional[TfidfVectorizer] = None\n",
        "        self._matrix = None\n",
        "        self._knowledge: List[KnowledgeChunk] = []        # docs + memories (as chunks)\n",
        "        self._last_payload: Optional[Dict[str, Any]] = None\n",
        "        self._prev_payload: Optional[Dict[str, Any]] = None\n",
        "\n",
        "        self._rebuild_knowledge_index()\n",
        "\n",
        "    # ---------- public: ingest / remember ----------\n",
        "\n",
        "    def ingest_document(self, text: str, source: str = \"user_doc\", max_chars: int = 1200) -> List[str]:\n",
        "        \"\"\"Ingest a large document once; we will retrieve only relevant chunks later.\"\"\"\n",
        "        chunks = chunk_text(text, max_chars=max_chars)\n",
        "        new_ids = []\n",
        "        for i, ch in enumerate(chunks):\n",
        "            cid = f\"doc_{uuid.uuid4().hex[:10]}\"\n",
        "            self.docs.append(\n",
        "                KnowledgeChunk(\n",
        "                    id=cid,\n",
        "                    kind=\"doc\",\n",
        "                    text=ch,\n",
        "                    source=source,\n",
        "                    metadata={\"chunk_index\": i, \"chunks_total\": len(chunks)},\n",
        "                )\n",
        "            )\n",
        "            new_ids.append(cid)\n",
        "        self._rebuild_knowledge_index()\n",
        "        return new_ids\n",
        "\n",
        "    def remember(self, text: str, category: str = \"other\", importance: int = 3, pinned: Optional[bool] = None):\n",
        "        \"\"\"Add a durable memory fact.\n",
        "\n",
        "        If pinned is None, we auto-pin high-importance identity/preferences.\n",
        "        \"\"\"\n",
        "        text = (text or \"\").strip()\n",
        "        if not text:\n",
        "            return\n",
        "\n",
        "        if pinned is None:\n",
        "            pinned = (category in {\"identity\", \"preference\", \"constraint\"} and importance >= 4)\n",
        "\n",
        "        mid = f\"mem_{uuid.uuid4().hex[:10]}\"\n",
        "        item = {\n",
        "            \"id\": mid,\n",
        "            \"kind\": \"memory\",\n",
        "            \"category\": category,\n",
        "            \"text\": text,\n",
        "            \"importance\": int(importance),\n",
        "            \"pinned\": bool(pinned),\n",
        "            \"created_at\": _now_iso(),\n",
        "        }\n",
        "        self.memories.append(item)\n",
        "\n",
        "        # enforce pinned cap (drop oldest low-importance pinned first)\n",
        "        self._enforce_pinned_cap()\n",
        "\n",
        "        self._rebuild_knowledge_index()\n",
        "\n",
        "    def forget(self, pattern: str) -> int:\n",
        "        \"\"\"Remove memories whose text matches a regex/pattern (case-insensitive).\"\"\"\n",
        "        pattern = (pattern or \"\").strip()\n",
        "        if not pattern:\n",
        "            return 0\n",
        "        rx = re.compile(pattern, re.IGNORECASE)\n",
        "        before = len(self.memories)\n",
        "        self.memories = [m for m in self.memories if not rx.search(m.get(\"text\", \"\"))]\n",
        "        removed = before - len(self.memories)\n",
        "        if removed:\n",
        "            self._rebuild_knowledge_index()\n",
        "        return removed\n",
        "\n",
        "    def reset_conversation(self, keep_memory: bool = True, keep_docs: bool = True):\n",
        "        self.summary = \"\"\n",
        "        self.recent_messages = []\n",
        "        self._prev_payload = None\n",
        "        self._last_payload = None\n",
        "        if not keep_memory:\n",
        "            self.memories = []\n",
        "        if not keep_docs:\n",
        "            self.docs = []\n",
        "        self._rebuild_knowledge_index()\n",
        "\n",
        "    # ---------- retrieval + context assembly ----------\n",
        "\n",
        "    def _enforce_pinned_cap(self):\n",
        "        pinned = [m for m in self.memories if m.get(\"pinned\")]\n",
        "        if len(pinned) <= self.pinned_memory_limit:\n",
        "            return\n",
        "        # sort pinned by (importance asc, created_at asc) and drop until within cap\n",
        "        pinned_sorted = sorted(pinned, key=lambda m: (m.get(\"importance\", 3), m.get(\"created_at\", \"\")))\n",
        "        to_drop = set(m[\"id\"] for m in pinned_sorted[: len(pinned) - self.pinned_memory_limit])\n",
        "        self.memories = [m for m in self.memories if m[\"id\"] not in to_drop]\n",
        "\n",
        "    def _rebuild_knowledge_index(self):\n",
        "        # represent memories as KnowledgeChunks so docs+memories can share the same TF-IDF index\n",
        "        mem_chunks: List[KnowledgeChunk] = []\n",
        "        for m in self.memories:\n",
        "            mem_chunks.append(\n",
        "                KnowledgeChunk(\n",
        "                    id=m[\"id\"],\n",
        "                    kind=\"memory\",\n",
        "                    text=f\"[{m['category'].upper()}] {m['text']}\",\n",
        "                    source=\"memory\",\n",
        "                    metadata={\"importance\": m.get(\"importance\", 3), \"pinned\": m.get(\"pinned\", False)},\n",
        "                )\n",
        "            )\n",
        "        self._knowledge = list(self.docs) + mem_chunks\n",
        "\n",
        "        if not self._knowledge:\n",
        "            self._vectorizer = None\n",
        "            self._matrix = None\n",
        "            return\n",
        "\n",
        "        texts = [k.text for k in self._knowledge]\n",
        "        # small + demo-friendly: rebuild from scratch\n",
        "        self._vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "        self._matrix = self._vectorizer.fit_transform(texts)\n",
        "\n",
        "    def retrieve(self, query: str, k: Optional[int] = None) -> List[Tuple[KnowledgeChunk, float]]:\n",
        "        \"\"\"Return top-k knowledge chunks relevant to `query`.\"\"\"\n",
        "        k = int(k or self.retrieved_snippets)\n",
        "        query = (query or \"\").strip()\n",
        "        if not query or not self._knowledge or self._vectorizer is None or self._matrix is None:\n",
        "            return []\n",
        "\n",
        "        qv = self._vectorizer.transform([query])\n",
        "        sims = cosine_similarity(qv, self._matrix).flatten()\n",
        "        # get best indices\n",
        "        best = sims.argsort()[::-1]\n",
        "        results: List[Tuple[KnowledgeChunk, float]] = []\n",
        "        for idx in best[: max(k * 3, k)]:  # scan a bit deeper for non-zero sims\n",
        "            score = float(sims[idx])\n",
        "            if score <= 0:\n",
        "                continue\n",
        "            results.append((self._knowledge[idx], score))\n",
        "            if len(results) >= k:\n",
        "                break\n",
        "        return results\n",
        "\n",
        "    def _build_context_packet(self, latest_user_message: str) -> Dict[str, Any]:\n",
        "        \"\"\"Create a structured context packet we can both show to the user and embed in instructions.\"\"\"\n",
        "        retrieved = self.retrieve(latest_user_message, k=self.retrieved_snippets)\n",
        "\n",
        "        pinned = [m for m in self.memories if m.get(\"pinned\")]\n",
        "        pinned_sorted = sorted(pinned, key=lambda m: (-m.get(\"importance\", 3), m.get(\"created_at\", \"\")))\n",
        "\n",
        "        # Memories that were retrieved by relevance (non-pinned too)\n",
        "        retrieved_mem = []\n",
        "        for chunk, score in retrieved:\n",
        "            if chunk.kind == \"memory\":\n",
        "                # look up original memory record\n",
        "                m = next((mm for mm in self.memories if mm[\"id\"] == chunk.id), None)\n",
        "                if m:\n",
        "                    retrieved_mem.append({**m, \"score\": score})\n",
        "\n",
        "        # Deduplicate: pinned wins\n",
        "        pinned_ids = {m[\"id\"] for m in pinned_sorted}\n",
        "        retrieved_mem = [m for m in retrieved_mem if m[\"id\"] not in pinned_ids]\n",
        "\n",
        "        retrieved_docs = []\n",
        "        for chunk, score in retrieved:\n",
        "            if chunk.kind == \"doc\":\n",
        "                retrieved_docs.append({\n",
        "                    \"id\": chunk.id,\n",
        "                    \"source\": chunk.source,\n",
        "                    \"score\": score,\n",
        "                    \"text\": chunk.text,\n",
        "                    \"metadata\": chunk.metadata,\n",
        "                })\n",
        "\n",
        "        packet = {\n",
        "            \"summary\": self.summary.strip(),\n",
        "            \"pinned_memory\": pinned_sorted,\n",
        "            \"retrieved_memory\": retrieved_mem,\n",
        "            \"retrieved_docs\": retrieved_docs,\n",
        "        }\n",
        "        return packet\n",
        "\n",
        "    def _render_context_packet_as_text(self, packet: Dict[str, Any]) -> str:\n",
        "        \"\"\"Convert packet -> a compact text block that goes into `instructions`.\"\"\"\n",
        "        parts = []\n",
        "        parts.append(\"## Context Packet (auto-generated each turn)\")\n",
        "\n",
        "        # summary\n",
        "        summary = packet.get(\"summary\") or \"\"\n",
        "        if summary.strip():\n",
        "            parts.append(\"### Running summary (older turns)\")\n",
        "            parts.append(summary.strip())\n",
        "        else:\n",
        "            parts.append(\"### Running summary (older turns)\")\n",
        "            parts.append(\"(none yet)\")\n",
        "\n",
        "        # memory\n",
        "        def _mem_line(m: Dict[str, Any]) -> str:\n",
        "            pin = \"ğŸ“Œ\" if m.get(\"pinned\") else \"\"\n",
        "            cat = m.get(\"category\", \"other\")\n",
        "            imp = m.get(\"importance\", 3)\n",
        "            return f\"- {pin}[{cat}, imp={imp}] {m.get('text','').strip()} (id={m.get('id')})\"\n",
        "\n",
        "        pinned = packet.get(\"pinned_memory\") or []\n",
        "        retrieved_mem = packet.get(\"retrieved_memory\") or []\n",
        "\n",
        "        parts.append(\"### Durable memory (preferences/goals/etc.)\")\n",
        "        if not pinned and not retrieved_mem:\n",
        "            parts.append(\"(none)\")\n",
        "\n",
        "        if pinned:\n",
        "            parts.append(\"Pinned:\")\n",
        "            parts.extend(_mem_line(m) for m in pinned)\n",
        "\n",
        "        if retrieved_mem:\n",
        "            parts.append(\"Relevant (retrieved):\")\n",
        "            parts.extend(_mem_line(m) for m in retrieved_mem)\n",
        "\n",
        "        # retrieved docs\n",
        "        docs = packet.get(\"retrieved_docs\") or []\n",
        "        parts.append(\"### Retrieved references (swap-in snippets)\")\n",
        "        if not docs:\n",
        "            parts.append(\"(none retrieved for this turn)\")\n",
        "        else:\n",
        "            for d in docs:\n",
        "                snippet = safe_truncate(d[\"text\"], max_chars=1200)\n",
        "                parts.append(f\"[doc_id={d['id']} | source={d['source']} | score={d['score']:.3f}]\\n\" + snippet)\n",
        "\n",
        "        # rules\n",
        "        parts.append(\"### Rules for using the Context Packet\")\n",
        "        parts.append(\"- Treat Durable memory + Running summary as authoritative for prior user preferences and decisions.\")\n",
        "        parts.append(\"- If the user request conflicts with memory, ask a clarifying question.\")\n",
        "        parts.append(\"- Retrieved references may be partial; cite them explicitly if you rely on them.\")\n",
        "        return \"\\n\\n\".join(parts).strip()\n",
        "\n",
        "    def _assemble_instructions_and_input(self, latest_user_message: str) -> Tuple[str, List[Dict[str, str]], Dict[str, Any]]:\n",
        "        packet = self._build_context_packet(latest_user_message)\n",
        "        packet_text = self._render_context_packet_as_text(packet)\n",
        "        instructions = self.base_instructions + \"\\n\\n\" + packet_text\n",
        "        input_items = list(self.recent_messages)  # shallow copy\n",
        "        debug = {\n",
        "            \"packet\": packet,\n",
        "            \"instructions\": instructions,\n",
        "            \"input\": input_items,\n",
        "        }\n",
        "        return instructions, input_items, debug\n",
        "\n",
        "    # ---------- token counting + compaction ----------\n",
        "\n",
        "    def _count_input_tokens(self, instructions: str, input_items: List[Dict[str, str]]) -> Optional[int]:\n",
        "        if not self.token_counting:\n",
        "            return None\n",
        "        # NOTE: this is an extra API call\n",
        "        try:\n",
        "            r = self.client.responses.input_tokens.count(\n",
        "                model=self.model,\n",
        "                instructions=instructions,\n",
        "                input=input_items,\n",
        "            )\n",
        "            return int(r.input_tokens)\n",
        "        except Exception as e:\n",
        "            print(\"[token_counting failed]\", repr(e))\n",
        "            return None\n",
        "\n",
        "    def _summarize_into_running_summary(self, messages_to_summarize: List[Dict[str, str]]) -> str:\n",
        "        transcript = format_messages(messages_to_summarize)\n",
        "\n",
        "        prompt = f\"\"\"Update the running summary for a long conversation.\n",
        "\n",
        "Existing running summary (may be empty):\n",
        "{self.summary.strip() or \"(none)\"}\n",
        "\n",
        "New transcript chunk to incorporate:\n",
        "{transcript}\n",
        "\n",
        "Write an updated running summary that:\n",
        "- keeps user goals, preferences, constraints\n",
        "- keeps decisions, plans, and important factual context\n",
        "- omits chit-chat and redundant detail\n",
        "- is written in plain text (bullets OK)\n",
        "\n",
        "Return ONLY the updated running summary.\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            resp = self.client.responses.create(\n",
        "                model=self.summarizer_model,\n",
        "                instructions=\"You summarize conversations into compact running summaries.\",\n",
        "                input=prompt,\n",
        "            )\n",
        "            return resp.output_text.strip()\n",
        "        except Exception as e:\n",
        "            print(\"[summarization failed]\", repr(e))\n",
        "            # fallback: naive compression\n",
        "            return safe_truncate((self.summary + \"\\n\" + transcript).strip(), max_chars=2000)\n",
        "\n",
        "    def _ensure_within_budget(self, latest_user_message: str) -> Tuple[str, List[Dict[str, str]], Dict[str, Any], Optional[int]]:\n",
        "        \"\"\"Try to keep the payload <= max_input_tokens by swapping context in/out.\"\"\"\n",
        "        # We'll keep trying in this order:\n",
        "        #   1) summarize older turns (swap transcript -> summary)\n",
        "        #   2) reduce retrieved snippet count (swap fewer docs in)\n",
        "        #   3) truncate the running summary if it grows too large\n",
        "\n",
        "        attempts = 0\n",
        "        local_retrieved_k = int(self.retrieved_snippets)\n",
        "\n",
        "        while True:\n",
        "            instructions, input_items, debug = self._assemble_instructions_and_input(latest_user_message)\n",
        "            tokens = self._count_input_tokens(instructions, input_items)\n",
        "\n",
        "            # If token counting is off or fails, we can't enforce strictly.\n",
        "            if tokens is None:\n",
        "                debug[\"note\"] = \"Token counting disabled/failed; not enforcing budget.\"\n",
        "                return instructions, input_items, debug, tokens\n",
        "\n",
        "            if tokens <= self.max_input_tokens:\n",
        "                debug[\"input_tokens\"] = tokens\n",
        "                debug[\"retrieved_k\"] = local_retrieved_k\n",
        "                return instructions, input_items, debug, tokens\n",
        "\n",
        "            attempts += 1\n",
        "            if attempts > 10:\n",
        "                debug[\"note\"] = \"Gave up enforcing budget after many attempts.\"\n",
        "                debug[\"input_tokens\"] = tokens\n",
        "                return instructions, input_items, debug, tokens\n",
        "\n",
        "            # (1) Summarize older messages if we have more than keep_last_messages\n",
        "            if len(self.recent_messages) > self.keep_last_messages:\n",
        "                to_summarize = self.recent_messages[: -self.keep_last_messages]\n",
        "                self.summary = self._summarize_into_running_summary(to_summarize)\n",
        "                self.recent_messages = self.recent_messages[-self.keep_last_messages :]\n",
        "                continue\n",
        "\n",
        "            # (2) Reduce retrieved snippets\n",
        "            if local_retrieved_k > 0:\n",
        "                local_retrieved_k -= 1\n",
        "                self.retrieved_snippets = local_retrieved_k\n",
        "                continue\n",
        "\n",
        "            # (3) Truncate summary as last resort\n",
        "            if self.summary and len(self.summary) > 1500:\n",
        "                self.summary = self.summary[-1500:]\n",
        "                continue\n",
        "\n",
        "            # (4) As a final fallback, drop oldest message(s)\n",
        "            if len(self.recent_messages) > 2:\n",
        "                self.recent_messages = self.recent_messages[2:]\n",
        "                continue\n",
        "\n",
        "            # nothing else to do\n",
        "            debug[\"note\"] = \"Unable to reduce further.\"\n",
        "            debug[\"input_tokens\"] = tokens\n",
        "            return instructions, input_items, debug, tokens\n",
        "\n",
        "    # ---------- auto memory extraction ----------\n",
        "\n",
        "    def _auto_memory_extract(self, user_text: str, assistant_text: str) -> MemoryDelta:\n",
        "        \"\"\"Use the model to extract durable memory facts (structured output).\"\"\"\n",
        "        existing = [\n",
        "            {\"id\": m[\"id\"], \"category\": m[\"category\"], \"text\": m[\"text\"], \"importance\": m[\"importance\"], \"pinned\": m[\"pinned\"]}\n",
        "            for m in self.memories\n",
        "        ]\n",
        "\n",
        "        prompt = f\"\"\"You are a memory extraction module for a chat assistant.\n",
        "\n",
        "Your job: extract ONLY durable, reusable facts worth remembering for future turns.\n",
        "Examples: identity, long-term goals, stable preferences, constraints, project specs.\n",
        "Do NOT include ephemeral details (e.g., jokes, one-off numbers, temporary states) unless the user says it is important.\n",
        "\n",
        "Existing memories (avoid duplicates):\n",
        "{json.dumps(existing, ensure_ascii=False)}\n",
        "\n",
        "Latest turn:\n",
        "USER: {user_text}\n",
        "ASSISTANT: {assistant_text}\n",
        "\n",
        "Return a JSON object matching the provided schema.\n",
        "\"\"\"\n",
        "\n",
        "        # Structured Outputs helper (parse into Pydantic model)\n",
        "        try:\n",
        "            resp = self.client.responses.parse(\n",
        "                model=self.memory_model,\n",
        "                input=[\n",
        "                    {\"role\": \"system\", \"content\": \"Extract durable memory facts in structured form.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ],\n",
        "                text_format=MemoryDelta,\n",
        "            )\n",
        "            return resp.output_parsed\n",
        "        except Exception as e:\n",
        "            print(\"[memory extract failed]\", repr(e))\n",
        "            return MemoryDelta(add=[])\n",
        "\n",
        "    def _apply_memory_delta(self, delta: MemoryDelta):\n",
        "        for fact in delta.add:\n",
        "            # normalize\n",
        "            cat = (fact.category or \"other\").strip().lower()\n",
        "            if cat not in {\"identity\", \"preference\", \"project\", \"constraint\", \"other\"}:\n",
        "                cat = \"other\"\n",
        "            txt = (fact.text or \"\").strip()\n",
        "            if not txt:\n",
        "                continue\n",
        "\n",
        "            # avoid near-duplicates\n",
        "            txt_l = txt.lower()\n",
        "            if any(txt_l == (m.get(\"text\", \"\").strip().lower()) for m in self.memories):\n",
        "                continue\n",
        "\n",
        "            self.remember(txt, category=cat, importance=int(fact.importance), pinned=None)\n",
        "\n",
        "    # ---------- main step + inspection ----------\n",
        "\n",
        "    def step(self, user_text: str) -> str:\n",
        "        \"\"\"One conversational turn.\"\"\"\n",
        "        user_text = (user_text or \"\").strip()\n",
        "        if not user_text:\n",
        "            return \"\"\n",
        "\n",
        "        self._prev_payload = self._last_payload\n",
        "\n",
        "        # Add user message to the rolling window\n",
        "        self.recent_messages.append({\"role\": \"user\", \"content\": user_text})\n",
        "\n",
        "        # Ensure we stay under budget before generating\n",
        "        instructions, input_items, debug, tokens = self._ensure_within_budget(user_text)\n",
        "\n",
        "        # Call the model\n",
        "        resp = self.client.responses.create(\n",
        "            model=self.model,\n",
        "            instructions=instructions,\n",
        "            input=input_items,\n",
        "        )\n",
        "        assistant_text = resp.output_text.strip()\n",
        "\n",
        "        # Add assistant message\n",
        "        self.recent_messages.append({\"role\": \"assistant\", \"content\": assistant_text})\n",
        "\n",
        "        # Auto-memory update\n",
        "        if self.auto_memory:\n",
        "            delta = self._auto_memory_extract(user_text, assistant_text)\n",
        "            self._apply_memory_delta(delta)\n",
        "\n",
        "        # Save payload so we can inspect it anytime\n",
        "        self._last_payload = {\n",
        "            \"model\": self.model,\n",
        "            \"instructions\": instructions,\n",
        "            \"input\": input_items,\n",
        "            \"input_tokens\": tokens,\n",
        "            \"debug\": debug,\n",
        "        }\n",
        "\n",
        "        if self.show_context_each_turn:\n",
        "            self.show_context(show_full=False)\n",
        "\n",
        "        return assistant_text\n",
        "\n",
        "    def preview_next_payload(self, next_user_message: str) -> Dict[str, Any]:\n",
        "        \"\"\"Show what would be sent if `next_user_message` were the next user turn.\"\"\"\n",
        "        instructions, input_items, debug = self._assemble_instructions_and_input(next_user_message)\n",
        "        tokens = self._count_input_tokens(instructions, input_items)\n",
        "        return {\n",
        "            \"model\": self.model,\n",
        "            \"instructions\": instructions,\n",
        "            \"input\": input_items,\n",
        "            \"input_tokens\": tokens,\n",
        "            \"debug\": debug,\n",
        "        }\n",
        "\n",
        "    def show_context(self, show_full: bool = True, max_chars: int = 4000):\n",
        "        \"\"\"Print the *exact* payload we last sent to the model.\"\"\"\n",
        "        if not self._last_payload:\n",
        "            print(\"(no model call yet)\")\n",
        "            return\n",
        "\n",
        "        p = self._last_payload\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"MODEL: {p.get('model')} | input_tokens={p.get('input_tokens')}\")\n",
        "        print(\"-\"*80)\n",
        "        print(\"INSTRUCTIONS (what we prepend every turn):\")\n",
        "        print(\"-\"*80)\n",
        "        instr = p.get(\"instructions\", \"\")\n",
        "        print(safe_truncate(instr, max_chars=max_chars if show_full else 1200))\n",
        "        print(\"-\"*80)\n",
        "        print(\"INPUT MESSAGES (recent chat window):\")\n",
        "        for m in p.get(\"input\", []):\n",
        "            role = m.get(\"role\")\n",
        "            content = m.get(\"content\", \"\")\n",
        "            print(f\"[{role}] {safe_truncate(content, max_chars=700 if not show_full else 2000)}\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    def diff_context(self):\n",
        "        \"\"\"Show a diff of instructions between the last two turns.\"\"\"\n",
        "        if not self._prev_payload or not self._last_payload:\n",
        "            print(\"(need at least two turns)\")\n",
        "            return\n",
        "        a = (self._prev_payload.get(\"instructions\") or \"\").splitlines(keepends=True)\n",
        "        b = (self._last_payload.get(\"instructions\") or \"\").splitlines(keepends=True)\n",
        "        diff = difflib.unified_diff(a, b, fromfile=\"prev_instructions\", tofile=\"curr_instructions\")\n",
        "        print(\"\".join(diff))\n",
        "\n",
        "    # ---------- interactive loop (optional) ----------\n",
        "\n",
        "    def chat(self):\n",
        "        \"\"\"Simple terminal-like chat loop (works in Colab).\"\"\"\n",
        "        help_text = \"\"\"Commands:\n",
        "  /help                 show this help\n",
        "  /context              print the last payload sent to the model\n",
        "  /diff                 diff the last two instruction payloads\n",
        "  /memory               list memories\n",
        "  /forget <pattern>     delete memories matching regex/pattern\n",
        "  /summary              print the running summary\n",
        "  /ingest               ingest a document (paste, end with a single line: END)\n",
        "  /reset                reset conversation (keeps memory+docs)\n",
        "  /reset_all            reset conversation + memory + docs\n",
        "  /exit                 leave chat\n",
        "\"\"\"\n",
        "\n",
        "        print(help_text)\n",
        "        while True:\n",
        "            user = input(\"you> \").strip()\n",
        "            if not user:\n",
        "                continue\n",
        "            if user in {\"/exit\", \"/quit\"}:\n",
        "                break\n",
        "            if user == \"/help\":\n",
        "                print(help_text)\n",
        "                continue\n",
        "            if user == \"/context\":\n",
        "                self.show_context(show_full=True)\n",
        "                continue\n",
        "            if user == \"/diff\":\n",
        "                self.diff_context()\n",
        "                continue\n",
        "            if user == \"/memory\":\n",
        "                if not self.memories:\n",
        "                    print(\"(no memories)\")\n",
        "                else:\n",
        "                    for m in sorted(self.memories, key=lambda x: (-x.get(\"importance\", 3), x.get(\"created_at\", \"\"))):\n",
        "                        pin = \"ğŸ“Œ\" if m.get(\"pinned\") else \"\"\n",
        "                        print(f\"{pin}{m['id']} [{m['category']}, imp={m['importance']}] {m['text']}\")\n",
        "                continue\n",
        "            if user.startswith(\"/forget \"):\n",
        "                pat = user[len(\"/forget \"):].strip()\n",
        "                n = self.forget(pat)\n",
        "                print(f\"Removed {n} memories.\")\n",
        "                continue\n",
        "            if user == \"/summary\":\n",
        "                print(self.summary.strip() or \"(none)\")\n",
        "                continue\n",
        "            if user == \"/reset\":\n",
        "                self.reset_conversation(keep_memory=True, keep_docs=True)\n",
        "                print(\"Conversation reset (memory+docs kept).\")\n",
        "                continue\n",
        "            if user == \"/reset_all\":\n",
        "                self.reset_conversation(keep_memory=False, keep_docs=False)\n",
        "                print(\"Conversation + memory + docs reset.\")\n",
        "                continue\n",
        "            if user == \"/ingest\":\n",
        "                print(\"Paste text to ingest. End with a single line: END\")\n",
        "                lines = []\n",
        "                while True:\n",
        "                    line = input()\n",
        "                    if line.strip() == \"END\":\n",
        "                        break\n",
        "                    lines.append(line)\n",
        "                text = \"\\n\".join(lines).strip()\n",
        "                if text:\n",
        "                    ids = self.ingest_document(text, source=\"pasted_doc\")\n",
        "                    print(f\"Ingested {len(ids)} chunks.\")\n",
        "                else:\n",
        "                    print(\"(no text ingested)\")\n",
        "                continue\n",
        "\n",
        "            # normal chat turn\n",
        "            assistant = self.step(user)\n",
        "            print(\"\\nassistant>\" , assistant, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cccf9a98",
      "metadata": {
        "id": "cccf9a98"
      },
      "source": [
        "## 4) Start chatting\n",
        "\n",
        "Create an engine, then run `engine.chat()`.\n",
        "\n",
        "Tip: try *long* conversations, and occasionally call:\n",
        "\n",
        "- `/context` to see the exact context being sent\n",
        "- `/diff` to see how the context packet changed between turns (what got swapped in/out)\n",
        "- `/ingest` to paste a large document and then ask questions about it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fa3ed1",
      "metadata": {
        "id": "24fa3ed1"
      },
      "outputs": [],
      "source": [
        "engine = ContextEngine(\n",
        "    client=client,\n",
        "    model=MODEL,\n",
        "    summarizer_model=SUMMARIZER_MODEL,\n",
        "    memory_model=MEMORY_MODEL,\n",
        "    max_input_tokens=MAX_INPUT_TOKENS,\n",
        "    keep_last_messages=KEEP_LAST_MESSAGES,\n",
        "    retrieved_snippets=RETRIEVED_SNIPPETS,\n",
        "    pinned_memory_limit=PINNED_MEMORY_LIMIT,\n",
        "    auto_memory=AUTO_MEMORY,\n",
        "    token_counting=TOKEN_COUNTING,\n",
        "    show_context_each_turn=SHOW_CONTEXT_EACH_TURN,\n",
        ")\n",
        "\n",
        "print(\"Engine ready. Run: engine.chat()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "engine.chat()"
      ],
      "metadata": {
        "id": "d7zmFFfF4zIA"
      },
      "id": "d7zmFFfF4zIA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "698e987e",
      "metadata": {
        "id": "698e987e"
      },
      "source": [
        "## 5) Optional: a tiny scripted demo\n",
        "\n",
        "Run this cell to see:\n",
        "\n",
        "- Document ingestion (large text stored out-of-context)\n",
        "- Retrieval swapping (only relevant snippets are inserted into the context packet)\n",
        "- Memory extraction (the assistant stores durable facts)\n",
        "- Context inspection (we print the last payload)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21403cfc",
      "metadata": {
        "id": "21403cfc"
      },
      "outputs": [],
      "source": [
        "# Ingest a \"large\" doc (for demo purposes)\n",
        "doc_text = \"\"\"Context engineering is the practice of carefully selecting what information goes into\n",
        "a model's context window at each turn. Because context windows are limited, long\n",
        "conversations require techniques like summarization, retrieval, and memory.\n",
        "\n",
        "Summarization swaps out older verbatim turns for a compact summary.\n",
        "Retrieval swaps in only the most relevant external snippets for the current query.\n",
        "Memory stores durable facts (preferences, goals) to reuse across the session.\n",
        "\"\"\"\n",
        "\n",
        "engine.ingest_document(doc_text, source=\"demo_doc\")\n",
        "\n",
        "# A couple turns\n",
        "print(engine.step(\"Hi! My name is Sam. Please remember I prefer concise bullet points.\"))\n",
        "print(engine.step(\"Can you explain context engineering?\"))\n",
        "\n",
        "# Inspect what we actually sent\n",
        "engine.show_context(show_full=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}